Splicing codes are computational models that attempt to predict splicing behaviour based on putative regulatory features (such as sequence motifs).
They were first introduced in the seminal paper by Barash et al 2010a, b. Their introduction was motivated by the recognition that splicing is highly condition-specific and regulated by the complex interaction of many factors in such a way that it is only feasible to model this behaviour computationally.
[Barash 2010] focus on cassette exons and attempt to predict the change in splicing behaviour for a given exon between different tissues. The quantitative measure PSI or $\Psi$ they introduced to describe splicing behaviour is still commonly used today:
$\Psi$ is defined as the proportion of transcripts out of all transcripts that contain a given exon \cite{psi}. Given a random transcript, PSI denotes the probability of a particular exon being included or excluded.
Similarly, $\Psi_5$ is defined as the number of transcripts containing a particular alternative 3' splice site for a fixed 5' splice site. $\Psi_3$ is defined analogously as the number of transcripts containing a particular alternative 5' splice site for a fixed 3' splice site. $\Psi_5$ and $\Psi_3$ are particularly interesting to model the competition between different alternative splice sites.
To quantify the change of splicing behaviour between conditions, these models predict the corresponding $\Delta \Psi$. They were able to find novel regulators of key genes associated with diseases and to predict how genetic variants will affect splicing. [take quotes from jha et al here] Input to the model are over 1000 known and unknown motifs and higher-level features (such as exon/intron lengths and phylogenetic conversation scores) selected partially from previous studies and partially from de novo searches.
Improving upon these first models, the second 'generation' of splicing codes used several common and uncommon machine learning algorithms such as multinomial logistic regression, support vector machines (SVM) and Bayesian Neural Networks (BNN) to predict changes in alternative splicing behaviour. [add citations here, partially from jha et al] Among these, BNNs were able to outperform the other methods when evaluated on a microarray dataset based on mouse data. In contrast to models from the first generation, BNNs based models only took in sequence information and very high-level features like tissue type which meant that the model was automatically able to learn relevant motifs from the data.
However, BNNs often rely on expensive sampling methods like Markov Chain Monte Carlo (MCMC) to be able to sample models from a posterior distribution. It can be challenging to scale these methods to larger datasets and a large number of hidden variables. [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4058935/]
As a result, the third 'generation' of splicing codes relies on deep learning models which can effectively make use of the large amount of data available with the advent of high-throughput RNA-sequencing technologies. First forays into using deep learning-based models were made by Leung 2014. Using a Deep Neural Network (DNN) with an autoencoder, they were able to improve upon the results achieved by a BNN model. Albeit Leung 2014 initially used a different dataset and a different task formulation than [BNN model], Jha 2017 were able to show that these improvements also lasted when directly comparing the models on the same dataset using the same task formulation. Furthermore, Jha 2017 developed a framework for integrating further experimental data, like data from CLIP-seq based measurements of in vivo splice factors bindings, into the model developed by Leung 2014. Adding these further features improved the explained variance in splicing behaviour between tissues, as measured by the R^2 score, by roughly further 5% to an overall average value of 43.4%.
Taking inspiration from advances in Natural Language Processing, [d2v paper] developed splicing codes based on the automated feature learning approach from word2vec and doc2vec. Developing two models, one based on doc2vec and a simple MLP, and one based on word2vec and the all-convolutional Inception architecture known from Computer Vision [quote], they were able to achieve an average R^2 score of 69.2% significantly improving upon the predictive power of previous models.
In contrast to these splicing codes which predict the (differential) inclusion frequency of an exon, a parallel strand of research focuses on splicing codes for distinguishing between constitutive and alternatively spliced exons. Concretely, for the first task the dataset the models are trained on only consists of alternatively spliced cassette exons and the models have to find features that are predictive of the exact inclusion rate of an exon.
For the second task, the dataset consists of alternatively spliced as well as constitutive exons and the models have to find features predictive for distinguishing between constitutive and alternatively spliced exons.
While there is a large overlap between these features, there are also differences.
For predicting the inclusion level of an exon, features from the cassette exon and the surrounding exons have shown to be relevant. [add quote from dsc] For predicting whether an exon is constitutive or not, features around the cassette exon itself have been reported to be the most critical. [add quotes from dsc here]
[possibly talk more about features used by dsc and what constitutive exons could use]
[Busch and Hertel] used 262 features extracted from an exon and it's two flanking introns to train an SVM-based splicing code for distinguishing between constitutive exons, cassette exons and exons with an alternative 5' or 3' splice site. The dataset used to train the model was based on roughly 4 million ESTs and known isoforms, as well as the alternative events, track (Alt Events) of the UCSU Genome Browser.
Their model achieved very impressive results with an AUC of roughly 0.94 when differentiating between rarely included and constitutive exons, but performance decreases to roughly 0.60 when distinguishing between frequently included and constitutive exons. [DSC] improved upon this work by using a deep learning model which was automatically able to learn relevant features from the raw sequence. Their model was based on a combination of convolutional blocks for feature extraction as well as an MLP for classification based on the extracted features. Training on a similar EST-based dataset, their model is significantly more robust when distinguishing between highly included cassette exons and constitutive exons with the AUC only dropping to ~0.85. When distinguishing between rarely included cassette and constitutive exons, it was still able to achieve an impressive AUC of ~0.92.
